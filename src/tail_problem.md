# Computability Theory and the "Tail Problem" in Quantum Physics

<!-- toc -->

## What Is Computability and What Has It Got to Do with Anything?

Computability Theory is a branch of mathematics that was briefly of intense interest to mathematicians in the early-to-mid 20th Century. Its two leading figures, Alonzo Church and Alan Turing, produced results that settled the specific question, "what makes a number computable?" In doing so, they ushered in the post-industrial "information age" because Turing's approach using thought experiments involving finite-state automata (popularly known as "Turing Machines") became the blueprint for modern digital computer hardware while Church's Lambda Calculus became the blueprint for all computer programming languages.

But what does the question about "computability" even mean? And why is it a question that still should be asked in the context of fields like Qunatum physics?

First, some history.

### How Euclid's Parallel Postulate Changed Everything

Once upon a time, mathematicians in the Western tradition regarded themselves as engaged in a science, not different in kind from chemistry or physics. In this way of regarding mathematical truth, the fact that \\( 2 + 2 = 4 \\) is rooted in empirical observation: if I hold two pebbles in my hand then pick up two more, I will be holding a total of four pebbles. This empiricism not only justified claims about which mathematical statements were true and which were not, it guided the way that mathematical problems were posed and solved.

Euclid's _Elements_ epitomizes this view. While providing the model for an axiomitic approach to mathematical proof, his actual method was firmly based in "constructions" -- empirically verifiable operations using a physical straight-edge and compass. Euclid's axioms and postulates all hearken directly to such empiricism. They are _nearly_ all very simple declarative statements of obsverable facts or simple steps in a construction. "To extend a point to form a line," "to extend a line to form a plane," and so on.

Then there is the fifth postulate, a.k.a. the parallel postulate. It looks nothing like the others. In Euclid's original formulation, it takes a paragraph rather than a short phrase and can be difficult to puzzle out without aid of a diagram. Even later attempts to produce simpler formulations did not detract from the degree to which it seems like an outlier compared to the other axioms and postulates.

> Euclid's fifth postulate:
>
> If a line segment intersects two straight lines forming two
> interior angles on the same side that are less than two right
> angles, then the two lines, if extended indefinitely, meet on
> that side on which the angles sum to less than two right angles.

From Euclid's time right into the start of the modern era, mathematicians were not happy about this. Over the course of millenia, countless attempts were made to show that the parallel postulate must actually be a theorem, derivable from simpler axioms that look more like the others in Euclid's book. While this produced a number of alternative formulations of the axiom, some of which may be easier to visual than others, none were a reduction to radically simpler axioms. For example:

> Alternative formulation of the fifth postulate:
>
> Given a line and a point not on that line, there is exactly
> one line that can be drawn through the given point that
> does not meet the given line when both lines are
> extended indefinitely in both directions.

After enough time and failed attempts, Western mathematicians eventually shifted their focus away from trying to prove that the parallel postulate was really a theorem. Instead they tried to form an understanding of why it is actually an axiom. By the early 19th Century, mathematical techniques had evolved to the point that allowed the following kind of indirect proof to be carried out:

1. Assume some different axiom that contradicts Euclid's parallel postulate in some specific way.
2. Show that the version of Euclid's constructions and proofs that can be carried out using this different version of the parallel postulate are just as _internally consistent_ as Euclid's "classical" proofs.

If the second step of the preceding proof can be accomplished for one or more variations from the parallel postulate assumed in the first step, then this proves that Euclid's fifth postulate was an axiom all along. If it were not an axiom but a theorem, altering it any way would emerge as a contradiction among the more basic axioms from which it is derived.

The key here is to separate the idea of "internal consistency" from "empirical truth." The so-called "Non-Eudclidean Geometries" invented by the likes of Bolyai, Lobachevsky, Riemann _et al._ each produce different theorems that contradict those described by Euclid, but as long as such Non-Euclidean axioms do not contradict themselves for a given version of the parallel postulate, they demonstrate that each such variation on the theme of "Geometry" is just as good, in some sense, as every other -- including Euclid's own. Carrying out proofs in Riemann's Elliptical Geometry is like doing Euclid-style constructions on the surface of a sphere where all lines (at least those lines forming "great circles" around the circumference) will eventually intersect. Lobachevsky's Hyperbolic Geometry assumes, on the other hand, that you can draw an infinite number of lines through any point that do not intersect some other line. Neither variation corresponds to what happens when, like Euclid, you use a straight-edge and compass on a flat surface. But neither Lobachevskian nor Riemannian Geometries produce _self_-contradictory results.

Mathematicians of their day rejoiced at Bolyai's, Lobachevsky's and Riemann's accomplishments for multiple reasons. They could finally regard the whole question pertaining to the status of the parallel postulate as satisfactorily settled. They could start playing with the new mathematical techiques developed for this enterprise, seeking additional areas of mathematical exploration where they might apply. But there was a reckoning to be had at the end of the celebration: in order to accept the highly desirable result, mathematicians had to fundamentally alter their understanding of their own profession. Mathematics became concerned with _validity_ (internal consistency) rather than (empirical) _truth_. Euclid's theorems do not merely form an interesting set of mutually consistent mathematical formulas. Euclid's reliance on "constructions" demonstrate that his theorems describe properties of objects and operations carried out in the real world. Non-Eudclidean Geometries have no such correspondence to observable reality (at least not at the scale at which human senses operate). Not only does every valid Non-Euclidean Geometry contradict Euclid's "common sense" Geometry, each contradicts every other. But if validity is the goal rather than correspondence to empirical truth, that does not matter at all. A "mathematical truth" was accepted as proven even though that required abandoning the need for, or ability to, assert anything at all about "empirical truth" solely on the basis of mathematics.

### Set Theory and Infinite Quantities

The change in outlook from truth to validity opened the way to vast new realms of mathematical enquiry. For example, right up to the middle of the 19th Century it was considered "out of bounds" for mathematicians to posit the existence of "completed infinities" in their calculations. After all, you can never hold an infinite number of pebbles in your hand. When Leibniz and Newton separately invented Calculus in the 17th Century, they each had to perform some mental gymnastics that allowed them to treat "infinitessimal" quantities as mathematically sound while still shying away from the mathematical inverses of infinitessimals, i.e. infinite quatities. As a foreshadowing of how the world of science and mathematics would react to Non-Euclidean Geometry a couple of centuries later, Leibniz' and Newton's contemporaries found Calculus too compelling and useful to ignore, while consciously repressing the cognitive dissonance required to embrace infinitessimals. That repression finally ended when the generation of mathematicians following the invention of Non-Euclidean Geometries fully embraced the consequences: go ahead and construct proofs based on infinite quantities or any other formerly forbidden category of mathematical object. So long as your results are internally consistent then your mathematics are valid. There may or may not be a "meta-mathematical theory" mapping your results to properties of the real world, as in the case of Euclidean Geometry. If so, that may be a pleasant side-effect of potential use to scientists and engineers, but such a corresondence to reality is no concern of "pure" math, in this new way of looking at things.

Simultaneously, the new emphasis on validity begged the question: what exactly is "validity," in the first place? Previously, it had been considered necessary and sufficient to demonstrate the correctness of a mathematical theory to show how it described properties of real things. Even abstrations like negative numbers were justified by treating them the way that bookkeepers treat balance sheets. Oddities like what happens when you try to divide any number by zero were regarded as special cases needing no further explanation than, "that can't happen in the real world, so just skip over it." But once approaches like those which used Non-Euclidean Geometries to prove something about classical Geometry, such empiricism was no longer necessary nor sufficient. The nature of mathematics and mathematical formulas in and of themselves suddenly became a topic of great interest to mathematicians. Thus were Symbolic Logic, Set Theory and Formal Linguistics born as areas of intense mathematical inquiry. Symbolic Logic is a mathematical system for determining whether or not a set of statements are mutually consistent, i.e. form a valid argument. Set Theory considers the mathematical properties of collections of "things" without regard to the properties of the things themselves. Formal Linguistics is the study of the properties of the sets of symbols and rules for combining them used to express mathematical formulas (or, later, computer programs) in meaningful and useful ways.

Georg Cantor used this new freedom and these novel approaches to consider the following question: is there more than one infinite quantity? If Cantor had been born even a generation earlier, he would have been laughed out of his doctoral program for even posing such a question. Infinite quantities were "right out" (to quote Monty Python) as mathematical objects in the first place, let alone the question of what mathematical properties they might have. But by Cantor's day, the world of academic mathematics was ready to give almost anything a go.

The school-yard understanding of "infinity" is that once you reach it, that's all there is. If a grade-schooler says to her best friend, "I love you," the friend may reply, "I love you more!" The natural response is, "I love you twice as much!" Which elicits, "I love you times 10!" Then "...times 100!" Eventually, one of the amicable combatants will end the game with, "...times infinity," to which, on the school yeard at least, there is no retort since (as every child understands) once you reach "infinity" you cannot count any higher. Cantor understood this as meaning that what most people thing of as \\(\infty\\) corresponds to the cardinality of the set of natural numbers. As you count up indefinitely from 1, you are defining the set of natural numbers by intention, as opposed to defining a set by extension, which requires an explicit list of all its members.

To understand Set Theory's terminology like "cardinality," "intension vs. extension" and so on, consider the letters of the Latin alphabet as a set of symbols. There are twenty-six of them as used in English, so the cardinality of this set is 26. ("Cardinality" can be loosely understood as "the number of members of a given set.") The set is defined by extension, as demonstrated by the "...now I've learned my A, B, C's..." chant every American grade-schooler is taught. I.e. what makes "A B C D E F G H I J K L M N O P Q R S T U V W X Y Z" the set of "letters of the alphabet" is simply the assertion that the given list _is_ the given set. This is what it means to define a set by extension. Only finite sets can be defined by extension, because only a finite number of items can be listed explicitly.

But what about a set like the natural numbers? Prior to Cantor's day, the answer would have been, "there is no such thing as the set of natural numbers because that would represent a completed infinity." But the new interest in things like Set Theory and the new freedom to explore valid theories whether or not they could be empirically verified allows one to define "the set of natural numbers" by _intension_. That is to define rules by which anything can be tested to determine whether or not it belongs in the set. So the set of natural numbers can be defined by intension using the following rule:

> Definition of the set of natural numbers by intention:
>
> Anything is a natural number if and only if it is the number 1
> (or 0, if you prefer) or it is the result of adding 1 to a
> natural number.

By the preceding rule, 2 is in the set of natural numbers because it is the result of 1 + 1. Given that, so is 3, because it is the result of 2 + 1. And so on. The number 1.5 is not a natural number because there is no way to produce it by adding 1 to any natural number. This allows "the set of natural numbers" to be well defined, but what about its cardinality? Cantor begins his exploration of infinite quantity simply by stipulating that the set of natural numbers when defined by intention has a cardinality, usually denoted using the last letter of the Greek alphabet, \\(\Omega\\). Since there is no limit to the number of times you can add 1 to a natural number, \\(\Omega\\) represents an infinite quantity. But is \\(\Omega\\) "all there is" when it comes to infinite quantities, as in the school-yard intuition regarding \\(\infty\\)?

Cantor's answer, as it turns out, is, "no, there are an infinite number of infinite quantities which differ in magnitude in the same way as the magnitudes of finite numbers differ." To prove this, we need to intoduce some additional jargon: "mapping" and "countability." A "mapping" is a rule by which elements in one set are associated with elements in another set. For finite sets defined by extension, such mappings can be defined by extension as well. A mapping can be represented as a set of ordered pairs where the first member of a pair is from one set and the second member of a pair is from the other set. An obvious mapping from natural numbers to letters of the Latin alphabet as used in English would be:

```
 1, A
 2, B
 3, C
 4, D
 5, E
 6, F
 7, G
 8, H
 9, I
10, J
11, K
12, L
13, M
14, N
15, O
16, P
17, Q
18, R
19, S
20, T
21, U
22, V
23, W
24, X
25, Y
26, Z
```

This is simply labeling each letter with its position in the conventional ordering when reciting the "A, B, C" song, which is also the normal sorting order when alphabetizing entries in a telephone directory, a book's index or the like. Another name for such a mapping is a "function" where the set from which the first member of each pair is selected is the "domain" and the set from which the second member of each pair is selected is called the "range." The significance of the "function" terminology will become apparent when discussing such mappings between sets with infinite cardinalities. Note that this particular mapping has some interesting properties, making it "one to one" (1:1): every element in each set corresponds to exactly one element in the other such that no element in either set is left out of the mapping and no element in either set corresponds to more than one element in the other. Only sets with the same cardinality ("number of elements") can have 1:1 mappings between them.

As with infinite sets themselves, mappings involving them must be defined using a rule for choosing an element from the range given an element from the domain. Where both sets consist of numbers, an obvious way of defining such a rule is as a mathematical formula, hence the use of the term "function" for such mappings. If such a mapping is 1:1, this means that the rule (function) is such that given a particular element from the domain, it will choose (compute) exactly one element from the range and it would be possible to define an inverse rule (function) choosing (computing) exactly one element in the domain given a particular element in the range. If such a 1:1 mapping is possible then the two infinite sets have the same cardinality (and that is why the term "cardinality" is used for sets and is not quite the same as "number of elements" in the sense that the number of elements in the set of letters of the alphabet is 26). Any set for which a 1:1 mapping is possible between that set and the set of natural numbers is said to be "countable" since one could "count" the elements in the given set by computing the element of that set that corresponds to any given natural number using the 1:1 function, i.e. producing an infinitely long list of pairs that looks similar to the one above for the 26 letters of the alphabet. The cardinality of every countable set is \\(\Omega\\).

For Cantor, the test of the school-yard intuition that once you reach \\(\infty\\), that is as big as a quantity can be thus became: is the cardinality of every infinite set equal to \\(\Omega\\), i.e. are all infinite sets countable? His answer was that while some infinite sets are countable, others are not. See <https://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument> for a description of his arugment that the set of real numbers is not countable. To summarize it briefly, assume that you have a countable set of real numbers between any two points on the number line. It is possible to apply a procedure for generating an infinite sequence of digits representing a real number that cannot be in your original list. Therefore there are (infinitely) more real numbers between any two points on the number line than there are natural numbers, no matter how close together you choose the end points of the segment of the number line to start with. If we accept Cantor's argument then the cardinality of the set of real numbers is infinitely larger than \\(\Omega\\), the cardinality of the set of natural numbers. And those are just the first two infinite magnitudes considered by mathematicians in the Western tradition.

### The Computability Problem

The definition of a countable set is that there is a 1:1 function that maps natural numbers to elements of that set. If the set of real numbers is not countable, that means that there are real numbers for which no such function is possible. In fact, a consequence of Cantor's arguments is that the percentage of real numbers for which such a mapping function is possible is so small compared to the real numbers denoted by all the points on the number line, the ones that do correspond to a countable set must be very special in some way.

Computability Theory attempts to answer the question, "what makes countable real numbers so special?" _Or to put it another way, "what makes a real number computable?"_ A consequence of regarding the problem according to that second formulation of the question is that almost no real numbers are computable, i.e. the percentage of points on the number line for which you have any hope of calculating their values is vanishingly small. Something must be very special about the particular real numbers whose values can be computed.

To justify the second formulation of the question of "computability," remember that for a 1:1 mapping to be shown to exist between two infinite sets, you must be able to produce a function which chooses exactly one element from one set given any particular element from the other. For practical purposes, such a function is going to be expressed as a mathematical formula. For countable sets, the domain of that function is the natural numbers. Cantor's arguments show that no such function can exist when the range of such a function is the set of real numbers. This has conventionally been interpreted to mean that if the set of all possible mathematical formulas for producing real numbers is countable, then it cannot produce all of the real numbers. Proofs in Formal Linguistics by Kurt Goedel and others show that the set of well-formed expressions in any useful mathematical "language" is countable. _Therefore, the conventional wisdom is that there are infinitely more real numbers than there are formulas for expressing computations and so most real numbers cannot be computed._

## Computability Theory to the Rescue

Computability Theory arose as an area of mathematical research as a direct result of the observation that there is something special about computable numbers. How does one go about characterizing what distinguishes a computable number from the vastly larger population of real numbers, generally? (Leaving aside, for now, the question of how "real" a number can be if it cannot be computed.) Starting in the early 20th Century, various mathematical philosophers and philosophically-inclined mathematicians worked on this question. The question was considered settled when the work of two such people, Alonzo Church and Alan Turing, converged.

Church's approach was purely formal. He invented a mathematical language, the Lambda Calculus, for defining any possible calculation. Church's thesis was that the set of computable numbers is the set of results of evaluating all well-formed formulas of the Lambda Calculus. This way of characterizing computability is very much in keeping with what had by then become the mainstream "all that matters is validity" view of mathematics.

Turing's approach started with thought experiments involving hypothetical "Turing Machines." In this way, his work actually fits well with the classical, "empiricist" view of mathematics. Turing described something called a finite-state automaton. A finite-state automaton is a theoretical mechanical device built according to certain rules and which produces a particular sequence of symbols (e.g. digits in the representation of a real number) as the result of its operation. If a Turing Machine could be constructed (which it could not be in reality because it would require an infinitely long paper tape), Turing's thesis is that the outputs of all possible such machines would be the set of computable numbers.

Computability Theory was considered settled and the world moved on to other interests when it was shown that Church's and Turing's theses are mathematically equivalent: the behavior of any possible Turing Machine can be described using a formula of the Lambda Calculus, and any formula of the Lambda Calculus can be used as the "blueprint" for a Turing Machine. (As a side-effect, Turing and Church had between them provided the mathematical model for programmable digital computers and for the programming languages used to control them, but that is another story.) But note something interesting: neither Church's nor Turing's work actually directly addresses what makes computable numbers different from all those other real-but-not-computable ones. Instead, they provide a means for characterizing an exhaustive set of computable numbers while saying nothing at all about non-computable ones other than that they are not generated by Turing Machines carrying out Lambda calculations.

### How Real Is a Number That Cannot Be Computed?

The last point made above is crucial. There is no question that Cantor's, Goedel's, Church's and Turing's arguments are valid. But remember that mathematical validity provides no guarantee regarding empirical reality. The idea of the "real number line" is very deeply ingrained in mathematical thinking in the Western tradition. Computability Theory's whole motivation stems from the assumption that the real number line is continuous and infinitely subdividable. In this model, each and every dimensionless point on the line corresponds to exactly one real number and between any two dimensionless points, no matter how close they are together, there are infinitely many more dimensionless points representing additional, unique real numbers.

While this works perfectly well for the purposes of "pure" mathematics whose only concern is validity, it does not take much understanding of chemistry and physics to know that such a number line cannot be a physical object. Any such object would be composed of some material. That material would be composed of some combination of molecules. Each molecule would be composed of some combination of atoms. Each atom would be composed of members of the "particle zoo" comprised of fermions, bosons, hadrons, the latter of which would be composed of quarks (and, unless something like M-Theory could ever be proven, that is as far as current understanding goes). The point is that there is a limit to the divisability of any physical substance. Divide a physical representation of the number line enough times, and it stops being the object with which you started and if this process of subdivision is continued far enough the whole thing dissovles into a cloud of "elementary" particles (and, probably, a mushroom cloud hovering over a zone of total destruction).

Similarly, no non-computable number could correspond to a measurable state of any physical object. No matter how high the resolution of some detection apparatus, it will be designed to test some hypothesis using engineering approaches both of which are bound to computable numbers. But remember the core insight of Qunatum physics: physical systems jump directly from quantum state to quantum state without passing through a continuum of states in between. If that is true, and no quantum state could ever be detected whose representation required use of a non-computable number, how would we ever know _and why should we even suspect_ that some of those states correspond to values that fall between the cracks of computable numbers? And if non-computable numbers cannot be detected physically nor characterized mathematically why should we take them seriously?

## The "Tail Problem"

Which brings us to one of the many supposed metaphysical challenges posed by Quantum Physics. Since phenomena at the quantum level are governed by probability, the assumption made by physicists trained in the Western tradition is that such probabilities have infinitely long "tails" -- there is always some chance, no matter how small, of some spectacularly unexpected state transition. But this view is based in part on the idea of the infinitely subdividable real number line and thus on the idea that non-computable numbers are real enough to have an effect that could be observed in the physical world. Computability Theory and Quantum Physics, itself, both suggest otherwise. If one assumes that numbers themselves are "quantized" (metaphorically, hence the quotes -- computable real numbers can still be regarded as "continuous" even if one dismisses the idea that a vast sea of non-computable values exist between every computable one) by their computability just as phyiscal systems are governed by quantized states, then the "tail problem" is greatly reduced. To compute a probability is to know its magnitude. While this by itself does not rule out very small probabilities, it does mean that definite calculations are possible without having to account, even conceptually, for any "wiggle room" left by the supposedly infinite number of non-computable probabilities between any two computable values which differ by minute magnitudes. Since macroscopic phenomena are composed of unimaginably vast agglomerations of particles for which quantum indeterminacy is relevant, sufficiently tiny probabilities by definition cancel one another out. If they did not, the calculations of the relative probabilities were simply incorrect to start with. At a very deep level, the laws of physics operate very similarly to casinos and government sponsored lotteries. At a casino, enormous jackpots are possible for any given player at any given moment but simply serve as cheap advertising when paid. The rate at which they are allowed to occur and the desired level of payout is baked into the probablity tables that drive the games and can easily be predicted far in advance. The "tail problem" is no different. Classical cause and effect is no more danger from quantum indeterminacy than a Vegas casino is in danger of going out of business because of too many jackpots in too short a time. In short, physicists would do well to leave metaphysics to the professionals in the Philosophy Department.